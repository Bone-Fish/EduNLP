{
  "architecture": "ElmoLMForPreTraining",
  "attention_unit_size": 256,
  "batch_first": true,
  "beta": 0.5,
  "dropout_rate": 0.5,
  "embedding_dim": 32,
  "fc_hidden_size": 512,
  "flat_cls_weight": 0.5,
  "head_dropout": 0.5,
  "hidden_size": 16,
  "num_classes_list": [
    3,
    27,
    34
  ],
  "num_total_classes": 64,
  "transformers_version": "4.16.2",
  "vocab_size": 100
}
