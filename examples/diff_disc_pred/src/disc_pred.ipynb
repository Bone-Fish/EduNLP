{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 区分度预估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 03:30:52.333651: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-28 03:30:52.381526: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-28 03:30:54.202449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from transformers import BertModel, TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from  torchmetrics import MeanAbsoluteError, PearsonCorrCoef, SpearmanCorrCoef\n",
    "import os\n",
    "from EduNLP.Pretrain import BertTokenizer \n",
    "import json\n",
    "from utils import  Dataset_bert, pre_disc\n",
    "from common import ROOT, DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/shangzi/anaconda3/envs/tgen/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "MAE = MeanAbsoluteError()\n",
    "PCC = PearsonCorrCoef()\n",
    "SCC = SpearmanCorrCoef()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据，定义预训练模型路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(ROOT, \"output/discrimination\") #模型保存路径\n",
    "pretrained_model_dir = \"../data/bert_math_768\" #bert路径，也可以更换为其他模型的路径，如disenqnet, roberta等\n",
    "train_items = pre_disc(os.path.join(DATA_DIR, \"train\", \"ctt_train.csv\")) #训练集\n",
    "val_items = pre_disc(os.path.join(DATA_DIR, \"test\", \"ctt_test.csv\")) #测试集"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminationPredictionOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    labels: torch.FloatTensor = None\n",
    "\n",
    "class BertPrediction(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_model_dir=None, config=None, classifier_dropout=0.5):\n",
    "\n",
    "        super(BertPrediction, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_dir) \n",
    "        \n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                content=None,\n",
    "                labels=None,\n",
    "                ):\n",
    "        \n",
    "        input_ids = content['input_ids'].squeeze(0)\n",
    "        attention_mask = content['attention_mask'].squeeze(0)\n",
    "        token_type_ids = content['token_type_ids'].squeeze(0)  \n",
    "        item_embed = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)['last_hidden_state'][:, 0, :]\n",
    "        logits = self.sigmoid(self.classifier(item_embed))\n",
    "        loss = F.mse_loss(logits.squeeze(0), labels)\n",
    "        return DiscriminationPredictionOutput(\n",
    "            loss = loss,\n",
    "            logits = logits,\n",
    "            labels = labels\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits = torch.as_tensor(pred.predictions[0]).squeeze(0)\n",
    "    logits = logits.view([logits.size()[0]],-1)\n",
    "    labels = torch.as_tensor(pred.label_ids)\n",
    "    pres = logits.numpy().tolist()\n",
    "    golds = labels.numpy().tolist()\n",
    "    ret = {\n",
    "        \"mae\": MAE(logits, labels),\n",
    "        \"mse\": mean_squared_error(golds,  pres),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(golds,  pres)),\n",
    "        \"pcc\": PCC(logits, labels),\n",
    "        \"scc\": SCC(logits, labels),\n",
    "        'ndcg': testdata_metrics(golds, pres).tolist(),\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "def testdata_metrics(diff, pred):\n",
    "    diff, pred = np.array(diff), np.array(pred)\n",
    "    ndcg = []\n",
    "    ndcg.append([ndcg_score([diff], [pred]), ndcg_score([diff], [pred], k=10), ndcg_score([diff], [pred], k=20), ndcg_score([diff], [pred], k=30)])\n",
    "    ndcg = np.mean(ndcg, axis=0)\n",
    "    return ndcg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练和测试相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    pass\n",
    "\n",
    "def train_disc_pred(\n",
    "                        output_dir,\n",
    "                        pretrained_model_dir,\n",
    "                        train_items=None,\n",
    "                        val_items=None,\n",
    "                        train_params=None):\n",
    "    model = BertPrediction(pretrained_model_dir=pretrained_model_dir) \n",
    "    tokenizer = BertTokenizer(add_special_tokens=True)\n",
    "  \n",
    "    if train_params is not None:\n",
    "        epochs = train_params['epochs'] if 'epochs' in train_params else 1\n",
    "        batch_size = train_params['batch_size'] if 'batch_size' in train_params else 64\n",
    "        save_steps = train_params['save_steps'] if 'save_steps' in train_params else 100\n",
    "        save_total_limit = train_params['save_total_limit'] if 'save_total_limit' in train_params else 2\n",
    "        logging_steps = train_params['logging_steps'] if 'logging_steps' in train_params else 5\n",
    "        gradient_accumulation_steps = train_params['gradient_accumulation_steps'] \\\n",
    "            if 'gradient_accumulation_steps' in train_params else 1\n",
    "        logging_dir = train_params['logging_dir'] if 'logging_dir' in train_params else f\"{ROOT}/log\"\n",
    "    else:\n",
    "        # default\n",
    "        epochs = 50\n",
    "        batch_size = 1\n",
    "        save_steps = 1000\n",
    "        save_total_limit = 2\n",
    "        logging_steps = 100\n",
    "        gradient_accumulation_steps = 1\n",
    "        logging_dir = f\"{ROOT}/log\"\n",
    "\n",
    "    train_dataset = Dataset_bert(train_items, tokenizer)\n",
    "    eval_dataset = Dataset_bert(val_items, tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy = \"steps\", # epoch\n",
    "        eval_steps=logging_steps*5,\n",
    "        \n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=save_total_limit,\n",
    "        \n",
    "        logging_steps=logging_steps,\n",
    "        logging_dir=logging_dir,\n",
    "\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=5e-5,\n",
    "    )\n",
    "\n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=train_dataset.collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    #trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_disc_pred(\n",
    "        output_dir,\n",
    "        pretrained_model_dir=pretrained_model_dir,\n",
    "        train_items=train_items,\n",
    "        val_items=val_items,\n",
    "        train_params= None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
