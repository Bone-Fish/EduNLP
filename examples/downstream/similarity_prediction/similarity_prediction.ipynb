{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下游任务Demo：相似度预估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import faiss\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 获取题目表征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取相似度预估下游任务题目数据，格式：每行一道题目文本\n",
    "with open({'path/to/your/data/math.tsv'}, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "ques = []\n",
    "for line in lines:\n",
    "    ques.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以DisenQNet为例\n",
    "\n",
    "from EduNLP.Pretrain import DisenQTokenizer\n",
    "from EduNLP.Vector import DisenQModel\n",
    "\n",
    "path = \"/path/to/disenqnet/checkpoint\"\n",
    "tokenizer = DisenQTokenizer.from_pretrained(path)\n",
    "t2v = DisenQModel(path, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_emb = []\n",
    "with np.errstate(all='raise'):\n",
    "    for i, text in enumerate(tqdm(ques)):\n",
    "        encodes = tokenizer([text], key=lambda x: x)\n",
    "        emb = t2v.infer_vector(encodes, key=lambda x: x[\"stem\"], vector_type=\"k\").detach().cpu().reshape(-1).numpy()\n",
    "        ques_emb.append(emb)\n",
    "ques_emb = np.array(ques_emb)\n",
    "ques_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cache/disenq_300_embs.pkl', 'wb') as f:\n",
    "    pickle.dump(ques_emb, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 相似度预估 Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "sim = pd.read_csv('/path/to/your/data/similarity.csv')\n",
    "test_id1 = []\n",
    "test_id2 = []\n",
    "labels = []\n",
    "for i, line in sim.iterrows():\n",
    "    id1, id2, _, _, _, sim = line\n",
    "    try:\n",
    "        idx1 = id1-1\n",
    "        idx2 = id2-1\n",
    "        score = sum([int(x) for x in sim.split('|')]) / 3\n",
    "        test_id1.append(idx1)\n",
    "        test_id2.append(idx2)\n",
    "        labels.append(score)\n",
    "    except:\n",
    "        print(id1, id2, score)\n",
    "np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_metrics(ques_emb):\n",
    "    ques_emb1 = ques_emb[test_id1]\n",
    "    ques_emb2 = ques_emb[test_id2]\n",
    "    cosine_scores = 1 - (paired_cosine_distances(ques_emb1, ques_emb2))\n",
    "    pearson_cosine, _ = pearsonr(labels, cosine_scores)\n",
    "    spearman_cosine, _ = spearmanr(labels, cosine_scores)\n",
    "    print(f'Pearson: {pearson_cosine:.4f}, Spearman: {spearman_cosine:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Step1中保存的题目表征\n",
    "with open('./cache/disenq_300_embs.pkl', 'rb') as f:\n",
    "    embs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ranking_metrics(embs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 相似度预估 Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Step1中保存的题目表征\n",
    "with open('./cache/disenq_300_embs.pkl', 'rb') as f:\n",
    "    embs = pickle.load(f)\n",
    "\n",
    "norm_embs = embs / (np.linalg.norm(embs, ord=2, axis=-1, keepdims=True) + 1e-12)\n",
    "norm_embs = norm_embs.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = norm_embs.shape[-1]\n",
    "param = 'IVF512,PQ15'\n",
    "measure = faiss.METRIC_L2\n",
    "index = faiss.index_factory(dim, param, measure)\n",
    "index.train(norm_embs)\n",
    "index.add(norm_embs)\n",
    "faiss.write_index(index, './index/disenq.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据并按照recall任务进行处理\n",
    "sim = pd.read_csv('/path/to/your/data/similarity.csv')\n",
    "query = {}\n",
    "for i, line in sim.iterrows():\n",
    "    id1, id2, _, _, _, sim = line\n",
    "    id1 = int(id1)\n",
    "    id2 = int(id2)\n",
    "    score = sum([int(x) for x in sim.split('|')]) / 3\n",
    "    if score >= 5:\n",
    "        if id1 in query:\n",
    "            query[id1].append((id2, score))\n",
    "        else:\n",
    "            query[id1] = [(id2, score)]\n",
    "        if id2 in query:\n",
    "            query[id2].append((id1, score))\n",
    "        else:\n",
    "            query[id2] = [(id1, score)]\n",
    "for k in query:\n",
    "    query[k].sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_metrics(query, result, p=100):\n",
    "    total_hr, total_ndcg = 0, 0\n",
    "    for k, v in query.items():\n",
    "        res = result[k][:p]\n",
    "        hit, dcg, idcg = 0, 0, 0\n",
    "        for i, (label, score) in enumerate(v):\n",
    "            idcg += (2 ** score - 1) / np.log2(i + 2)\n",
    "            if label in res:\n",
    "                hit += 1\n",
    "                dcg += (2 ** score - 1) / np.log2(res.index(label) + 2)\n",
    "        total_hr += (hit / len(v))\n",
    "        total_ndcg += (dcg / idcg)\n",
    "    print(f'HR@{p}: {total_hr / len(query):.4f}, NDCG@{p}: {total_ndcg / len(query):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time = 0\n",
    "for _ in range(5):\n",
    "    result = {}\n",
    "    total_time = 0\n",
    "    for k in tqdm(query):\n",
    "        idx = k-1\n",
    "        start = time.time()\n",
    "        _, idxs = index.search(norm_embs[idx].reshape(1, -1), 101)\n",
    "        end = time.time()\n",
    "        total_time += (end - start) * 1000\n",
    "        res_ids = idxs.tolist()[0]\n",
    "        if idx in res_ids:\n",
    "            res_ids.remove(idx)\n",
    "        result[k] = []\n",
    "        for i in res_ids[:100]:\n",
    "            try:\n",
    "                result[k].append(i+1)\n",
    "            except:\n",
    "                pass\n",
    "    print('Average time: ', total_time / len(query))\n",
    "    avg_time += total_time / len(query)\n",
    "    compute_recall_metrics(query, result, 10)\n",
    "    compute_recall_metrics(query, result, 20)\n",
    "    compute_recall_metrics(query, result, 30)\n",
    "    compute_recall_metrics(query, result, 50)\n",
    "    compute_recall_metrics(query, result, 100)\n",
    "print(avg_time / 5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
