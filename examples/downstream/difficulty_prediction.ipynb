{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 难度预估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from transformers import BertModel, TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from  torchmetrics import MeanAbsoluteError, PearsonCorrCoef, SpearmanCorrCoef\n",
    "import os\n",
    "from EduNLP.Pretrain import BertTokenizer \n",
    "from EduNLP.ModelZoo.base_model import BaseModel\n",
    "import json\n",
    "from utils import Dataset_bert, load_json, get_val, get_train\n",
    "\n",
    "ROOT = os.path.dirname(os.path.dirname(__file__))\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/shangzi/anaconda3/envs/tgen/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "MAE = MeanAbsoluteError()\n",
    "PCC = PearsonCorrCoef()\n",
    "SCC = SpearmanCorrCoef()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据，定义预训练模型路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_json] start : /data/shangzi/edunlp/gaokao-prediction/data/train/高中数学.json\n",
      "[load_json] num = 3600, open_path = /data/shangzi/edunlp/gaokao-prediction/data/train/高中数学.json\n",
      "[load_json] start : /data/shangzi/edunlp/gaokao-prediction/data/test/高中数学paper.json\n",
      "[load_json] num = 7, open_path = /data/shangzi/edunlp/gaokao-prediction/data/test/高中数学paper.json\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"output/difficulty\" #模型保存路径\n",
    "pretrained_model_dir = os.path.join(DATA_DIR, \"bert_math_768\") #bert路径，也可以更换为其他模型的路径，如disenqnet, roberta等\n",
    "train_data = load_json(os.path.join(DATA_DIR, \"train\", \"高中数学.json\")) #训练集\n",
    "train_items = get_train(train_data)\n",
    "val_data = load_json(os.path.join(DATA_DIR, \"test\", \"高中数学paper.json\")) #测试集\n",
    "val_items, val_gap = get_val(val_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifficultyPredictionOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    labels: torch.FloatTensor = None\n",
    "\n",
    "class BertPrediction(BaseModel):\n",
    "    \n",
    "    def __init__(self, pretrained_model_dir=None, classifier_dropout=0.5):\n",
    "        super(BertPrediction, self).__init__() \n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_dir)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        # print(hidden_size)\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                content=None,\n",
    "                labels=None,\n",
    "                ):\n",
    "        input_ids = content['input_ids']\n",
    "        attention_mask = content['attention_mask']\n",
    "        token_type_ids = content['token_type_ids']\n",
    "        item_embed = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)['last_hidden_state'][:, 0, :]\n",
    "        logits = self.sigmoid(self.classifier(item_embed))\n",
    "        loss = F.mse_loss(logits.squeeze(0), labels)\n",
    "        return DifficultyPredictionOutput(\n",
    "            loss = loss,\n",
    "            logits = logits,\n",
    "            labels = labels\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits = torch.as_tensor(pred.predictions[0]).squeeze(0)\n",
    "    logits = logits.view([logits.size()[0]],-1)\n",
    "    labels = torch.as_tensor(pred.label_ids)\n",
    "    print(\"logits\", logits)\n",
    "    print(\"labels\", labels)\n",
    "    pres = logits.numpy().tolist()\n",
    "    golds = labels.numpy().tolist()\n",
    "    ret = {\n",
    "        \"mae\": MAE(logits, labels),\n",
    "        \"mse\": mean_squared_error(golds,  pres),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(golds,  pres)),\n",
    "        \"pcc\": PCC(logits, labels),\n",
    "        \"scc\": SCC(logits, labels),\n",
    "        'ndcg': testdata_metrics(val_gap, golds, pres).tolist(),\n",
    "    }\n",
    "    return ret\n",
    "def testdata_metrics(val_gap, diff, pred):\n",
    "    diff, pred = np.array(diff), np.array(pred)\n",
    "    idx = np.where(diff>0)[0]\n",
    "    ndcg = []\n",
    "    for s, e in val_gap:\n",
    "        _diff, _pred = diff[s:e], pred[s:e]\n",
    "        if _diff[0]==-1:\n",
    "            _diff = [i+1 for i in range(len(_diff))]\n",
    "        ndcg.append([ndcg_score([_diff], [_pred]), ndcg_score([_diff], [_pred], k=10), ndcg_score([_diff], [_pred], k=20), ndcg_score([_diff], [_pred], k=30)])\n",
    "    ndcg = np.mean(ndcg, axis=0)\n",
    "    return ndcg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练和测试相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    pass\n",
    "\n",
    "def train_diff_pred(\n",
    "                        output_dir,\n",
    "                        pretrained_model_dir,\n",
    "                        train_items=None,\n",
    "                        val_items=None,\n",
    "                        test_items=None,\n",
    "                        train_params=None):\n",
    "    model = BertPrediction(pretrained_model_dir=pretrained_model_dir)\n",
    "    tokenizer = BertTokenizer(add_special_tokens=True)\n",
    "    # training parameters\n",
    "    if train_params is not None:\n",
    "        epochs = train_params['epochs'] if 'epochs' in train_params else 1\n",
    "        batch_size = train_params['batch_size'] if 'batch_size' in train_params else 64\n",
    "        save_steps = train_params['save_steps'] if 'save_steps' in train_params else 100\n",
    "        save_total_limit = train_params['save_total_limit'] if 'save_total_limit' in train_params else 2\n",
    "        logging_steps = train_params['logging_steps'] if 'logging_steps' in train_params else 5\n",
    "        gradient_accumulation_steps = train_params['gradient_accumulation_steps'] \\\n",
    "            if 'gradient_accumulation_steps' in train_params else 1\n",
    "        logging_dir = train_params['logging_dir'] if 'logging_dir' in train_params else f\"{ROOT}/log\"\n",
    "    else:\n",
    "        # default\n",
    "        epochs = 50\n",
    "        batch_size = 1\n",
    "        save_steps = 1000\n",
    "        save_total_limit = 2\n",
    "        logging_steps = 100\n",
    "        gradient_accumulation_steps = 1\n",
    "        logging_dir = f\"{ROOT}/log\"\n",
    "\n",
    "\n",
    "    train_dataset = Dataset_bert(train_items, tokenizer)\n",
    "    eval_dataset = Dataset_bert(val_items, tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy = \"steps\", # epoch\n",
    "        eval_steps=logging_steps*5,\n",
    "        \n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=save_total_limit,\n",
    "        \n",
    "        logging_steps=logging_steps,\n",
    "        logging_dir=logging_dir,\n",
    "\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=5e-5,\n",
    "    )\n",
    "\n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=train_dataset.collate_fn,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diff_pred(\n",
    "        output_dir,\n",
    "        pretrained_model_dir,\n",
    "        train_items=train_items,\n",
    "        val_items=val_items,\n",
    "        train_params= None\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
