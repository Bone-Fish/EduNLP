{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下游任务Demo：质量评估"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer as HfBertTokenizer\n",
    "import torch\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class IdsDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 mode,\n",
    "                 data_path=\"../data\",\n",
    "                 tokenizer=None,\n",
    "                 ):\n",
    "        self.mode = mode\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.data['answers']=self.data['answers'].fillna('')\n",
    "        self.data = self.data.to_dict(orient=\"records\") # [:20]\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.preprocess()\n",
    "\n",
    "    def preprocess(self):\n",
    "        for item in tqdm(self.data):\n",
    "            return_tensors = None if isinstance(self.tokenizer, HfBertTokenizer) else False\n",
    "\n",
    "            answers_encodings = self.tokenizer(item['contexts'], truncation=True, padding=True, max_length=512, return_tensors=return_tensors)\n",
    "            contexts_encodings = self.tokenizer(item['answers'], truncation=True, padding=True, max_length=512, return_tensors=return_tensors)\n",
    "            item[\"answers_encodings\"] = answers_encodings\n",
    "            item[\"contexts_encodings\"] = contexts_encodings\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def collate_fn(self, batch_data):\n",
    "        first =  batch_data[0]\n",
    "        batch = {\n",
    "            k: [item[k] for item in batch_data] for k in first.keys()\n",
    "        }\n",
    "        batch[\"answers_encodings\"] = self.tokenizer.batch_pad(batch[\"answers_encodings\"], return_tensors=True)\n",
    "        batch[\"contexts_encodings\"] = self.tokenizer.batch_pad(batch[\"contexts_encodings\"], return_tensors=True)\n",
    "        batch[\"score\"] = torch.as_tensor(batch[\"score\"])\n",
    "        batch[\"label\"] = torch.as_tensor(batch[\"label\"])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以 Bert 为例\n",
    "from EduNLP.Pretrain import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(path=\"/path/to/bert/checkpoint\")\n",
    "trainData = IdsDataset(mode='train', data_path=\"/path/to/train.csv\", tokenizer=tokenizer)\n",
    "validData = IdsDataset(mode='valid', data_path=\"/path/to/valid.csv\", tokenizer=tokenizer)\n",
    "testData = IdsDataset(mode='test', data_path=\"/path/to/test.csv\", tokenizer=tokenizer)\n",
    "\n",
    "train_Dataloader = DataLoader(trainData, shuffle=True, num_workers=0, pin_memory=True, collate_fn=trainData.collate_fn)\n",
    "valid_Dataloader = DataLoader(validData, shuffle=True, num_workers=0, pin_memory=True, collate_fn=validData.collate_fn)\n",
    "test_Dataloader = DataLoader(testData, shuffle=True, num_workers=0, pin_memory=True, collate_fn=testData.collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 质量评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from EduNLP.ModelZoo.base_model import BaseModel\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "class Global_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiModal global layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_unit, output_unit, hidden_size, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_unit, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, output_unit),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class QualityLoss(nn.Module):\n",
    "    def __init__(self, mode='train'):\n",
    "        super(QualityLoss, self).__init__()\n",
    "        if mode=='train':\n",
    "            self.classify_loss_fn = nn.CrossEntropyLoss()\n",
    "            self.logits_loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            self.classify_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "            self.logits_loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def forward(self, pred_score, pred_label,score, label, lamb=0.5):\n",
    "        # Loss\n",
    "        score_loss = self.logits_loss_fn(pred_score, score.float())\n",
    "        label_loss = self.classify_loss_fn(pred_label, label)\n",
    "        losses = score_loss*lamb + label_loss*(1-lamb)\n",
    "        return losses\n",
    "    \n",
    "    \n",
    "class TrainForQualityOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    score_logits: torch.FloatTensor = None\n",
    "    label_logits: torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class QualityModel(BaseModel):\n",
    "    def __init__(self, pretrained_model_type=\"bert\", pretrained_model_dir=None, emb_mode=\"index\", hidden_size=None, num_labels=3, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.pretrained_model_type = pretrained_model_type\n",
    "        self.emb_mode = emb_mode\n",
    "        self.num_labels = num_labels\n",
    "        if emb_mode == \"index\":\n",
    "            assert hidden_size is None\n",
    "            self.bert = BertModel.from_pretrained(pretrained_model_dir)\n",
    "            self.hidden_size = self.bert.config.hidden_size # 768\n",
    "        else: # vector\n",
    "            assert hidden_size is not None\n",
    "            self.hidden_size = hidden_size\n",
    "\n",
    "        self.score_decoder = Global_Layer(input_unit=self.hidden_size*2,\n",
    "                                            output_unit=1,\n",
    "                                            hidden_size=self.hidden_size,\n",
    "                                            dropout_rate=dropout_rate)\n",
    "        self.label_decoder = Global_Layer(input_unit=self.hidden_size*2,\n",
    "                                            output_unit=num_labels,\n",
    "                                            hidden_size=self.hidden_size,\n",
    "                                            dropout_rate=dropout_rate)        \n",
    "        self.criterion = QualityLoss()\n",
    "\n",
    "        self.config = {k: v for k, v in locals().items() if k not in [\"self\", \"__class__\", \"bert\"]}\n",
    "        self.config['architecture'] = 'QualityModel'\n",
    "\n",
    "    def forward(self,\n",
    "                context_vectors=None,\n",
    "                answer_vectors=None,\n",
    "                contexts_encodings=None,\n",
    "                answers_encodings=None,\n",
    "                score=None,\n",
    "                label=None,\n",
    "                **argv,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        batch_sentences : [batch_size, seq]\n",
    "        \"\"\"\n",
    "        if self.emb_mode == \"index\":\n",
    "            if self.pretrained_model_type in [\"bert\", \"roberta\"]:\n",
    "                contexts_encoder_out = self.bert(**contexts_encodings)\n",
    "                answers_encoder_out = self.bert(**answers_encodings)\n",
    "                context_vectors = contexts_encoder_out[1] # [batch_size,  hidden_size]\n",
    "                answer_vectors = answers_encoder_out[1] # [batch_size,  hidden_size]\n",
    "\n",
    "            elif self.pretrained_model_type == \"jiuzhang\":\n",
    "                contexts_encoder_out = self.bert(\n",
    "                                                input_ids=contexts_encodings[\"input_ids\"],\n",
    "                                                attention_mask=contexts_encodings[\"attention_mask\"],\n",
    "                                                )\n",
    "                answers_encoder_out = self.bert(\n",
    "                                                input_ids=answers_encodings[\"input_ids\"],\n",
    "                                                attention_mask=answers_encodings[\"attention_mask\"],\n",
    "                                                )\n",
    "                context_vectors = contexts_encoder_out[\"last_hidden_state\"][:, 0, :]\n",
    "                answer_vectors = answers_encoder_out[\"last_hidden_state\"][:, 0, :]\n",
    "            \n",
    "            elif self.pretrained_model_type == \"disenq\":\n",
    "                contexts_encoder_out = self.bert(**contexts_encodings)\n",
    "                answers_encoder_out = self.bert(**answers_encodings)\n",
    "                context_vectors = contexts_encoder_out[1]\n",
    "                answer_vectors = answers_encoder_out[1]\n",
    "        else:\n",
    "            assert context_vectors is not None and answer_vectors is not None\n",
    "        pooler_state = torch.cat([context_vectors, answer_vectors],dim=-1)\n",
    "        score_logits = self.score_decoder(pooler_state).squeeze(-1)\n",
    "        label_logits = self.label_decoder(pooler_state)\n",
    "        \n",
    "        loss = None\n",
    "        if score is not None and label is not None:\n",
    "            loss = self.criterion(score_logits, label_logits, score, label, lamb=0.5)\n",
    "\n",
    "        return TrainForQualityOutput(\n",
    "            loss=loss,\n",
    "            score_logits=score_logits,\n",
    "            label_logits=label_logits,\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config_path, **kwargs):\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as rf:\n",
    "            model_config = json.load(rf)\n",
    "            model_config.update(kwargs)\n",
    "            return cls(\n",
    "                pretrained_model_dir=model_config[\"pretrained_model_dir\"],\n",
    "                emb_mode=model_config[\"emb_mode\"],\n",
    "                hidden_size=model_config[\"hidden_size\"],\n",
    "                num_labels=model_config[\"num_labels\"],\n",
    "                dropout_rate=model_config[\"dropout_rate\"],\n",
    "                pretrained_model_type=model_config[\"pretrained_model_type\"]\n",
    "            )\n",
    "    \n",
    "    def save_config(self, config_dir):\n",
    "        config_path = os.path.join(config_dir, \"config.json\")\n",
    "        with open(config_path, \"w\", encoding=\"utf-8\") as wf:\n",
    "            json.dump(self.config, wf, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import MyTrainer\n",
    "\n",
    "# Initial model\n",
    "checkpoint_dir=\"your/checkpoint_dir\"\n",
    "device = \"cuda:0\"\n",
    "model = QualityModel.from_pretrained(checkpoint_dir).to(device)\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    ")\n",
    "trainer.train(train_Dataloader, valid_Dataloader)\n",
    "trainer.valid(valid_Dataloader)\n",
    "trainer.valid(test_Dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edunlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
