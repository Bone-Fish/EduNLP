{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/qlh/anaconda3/envs/py39/lib/python3.9/site-packages/js2py/constructors/time_helpers.py:10: UserWarning: Please install or fix tzlocal library (pip install tzlocal) in order to make Date object work better. Otherwise I will assume DST is in effect all the time\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import codecs\n",
    "from EduNLP.Pretrain import QuesNetTokenizer, pretrain_quesnet\n",
    "from EduNLP.Vector import T2V\n",
    "from EduNLP.I2V import QuesNet, get_pretrained_i2v\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练自己的QuesNet模型\n",
    "## 1. 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置你的数据路径和输出路径\n",
    "BASE_DIR = \"../..\"\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/static/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/examples/test_model/quesnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data():\n",
    "    _data = []\n",
    "    data_path = os.path.join(data_dir, \"standard_luna_data.json\")\n",
    "    with codecs.open(data_path, encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            _data.append(json.loads(line))\n",
    "    return _data\n",
    "\n",
    "raw_data = raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 训练Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 134605.39it/s]\n",
      "[EduNLP, INFO] save meta information know_name: 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save words(trim_min_count=3): 64/251 = 0.2550                  with frequency 696/929=0.7492\n",
      "vocab_size:  79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = QuesNetTokenizer(meta=['know_name'], max_length=50,\n",
    "                             img_dir=os.path.join(data_dir, \"quesnet_img\"))\n",
    "\n",
    "# 设置词表\n",
    "tokenizer.set_vocab(raw_data, key=lambda x: x['ques_content'], trim_min_count=3, silent=False)\n",
    "\n",
    "print(\"vocab_size: \", tokenizer.vocab_size)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存tokenizer\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练QuesNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1268095/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2854330761.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1268095/2854330761.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/data/qlh/EduNLP/EduNLP/Pretrain/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">quesnet_vec.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">759</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pretrain_quesnet</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">756 │   │   │   </span>**tokenizer_params,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">757 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">758 │   │   # token_items =</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>759 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tokenizer.set_vocab(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">760 │   │   │   </span>items=train_items,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">761 │   │   │   </span>key=data_formation[<span style=\"color: #808000; text-decoration-color: #808000\">\"ques_content\"</span>],                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">762 │   │   │   </span>trim_min_count=data_params.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"trim_min_count\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>),                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/data/qlh/EduNLP/EduNLP/Pretrain/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">quesnet_vec.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">317</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">set_vocab</span>                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">314 │   │   </span>symbol=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 │   │   </span>silent=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">316 │   </span>):                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>317 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>token_items = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.set_text_vocab(                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">318 │   │   │   </span>items,                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 │   │   │   </span>key=key,                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">320 │   │   │   </span>lower=lower,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/data/qlh/EduNLP/EduNLP/Pretrain/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">quesnet_vec.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">300</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">set_text_vocab</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">297 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">the lower bound number for adding a word into vocabulary, by default 1</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">298 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">silent</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">299 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>300 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>token_items = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenize(items, key=key, symbol=symbol) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> do_tokenize <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> [   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">301 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.vocab.set_vocab(corpus_items=token_items, trim_min_count=trim_min_count, lo   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">302 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.stoi[<span style=\"color: #808000; text-decoration-color: #808000\">'word'</span>] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.vocab.token_to_idx                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">303 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.itos[<span style=\"color: #808000; text-decoration-color: #808000\">'word'</span>] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.vocab.idx_to_token                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/data/qlh/EduNLP/EduNLP/Pretrain/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pretrian_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">299</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">tokenize</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">296 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(items, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(items, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>):                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">297 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._tokenize(items, key=key, **kwargs)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">298 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>299 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._tokenize(item, key=key, **kwargs) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> item <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> items]             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">300 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">301 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">encode</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, items: Tuple[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>, List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>], List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>]], key=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: x, **   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">302 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(items, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(items, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>):                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/data/qlh/EduNLP/EduNLP/Pretrain/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pretrian_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">299</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">296 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(items, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(items, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>):                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">297 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._tokenize(items, key=key, **kwargs)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">298 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>299 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._tokenize(item, key=key, **kwargs) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> item <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> items]             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">300 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">301 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">encode</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, items: Tuple[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>, List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>], List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>]], key=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: x, **   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">302 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(items, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(items, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>):                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/data/qlh/EduNLP/EduNLP/Pretrain/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pretrian_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">317</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_tokenize</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">314 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">NotImplementedError</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">316 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_tokenize</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, item: Tuple[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>], key=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: x, **argv):                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>317 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>token_item = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.text_tokenizer._tokenize(item, key=key, **argv)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">318 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(token_item) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 │   │   │   </span>token_item = [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.vocab.unk_token]                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">320 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(token_item) &gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.max_length:                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/data/qlh/EduNLP/EduNLP/Tokenizer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenizer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">70</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_tokenize</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 67 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">determine how to get the text of item, by default lambdax: x</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 68 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 69 │   │   </span>symbol = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.symbol <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> symbol <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> symbol                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 70 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> tokenize(seg(key(item), symbol=symbol, figures=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.figures),               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 71 │   │   │   │   │   │   </span>**<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenization_params, **kwargs).tokens                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 72 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 73 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span> object is not callable\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1268095/\u001b[0m\u001b[1;33m2854330761.py\u001b[0m:\u001b[94m17\u001b[0m in \u001b[92m<module>\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1268095/2854330761.py'\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/data/qlh/EduNLP/EduNLP/Pretrain/\u001b[0m\u001b[1;33mquesnet_vec.py\u001b[0m:\u001b[94m759\u001b[0m in \u001b[92mpretrain_quesnet\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m756 \u001b[0m\u001b[2m│   │   │   \u001b[0m**tokenizer_params,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m757 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m758 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# token_items =\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m759 \u001b[2m│   │   \u001b[0mtokenizer.set_vocab(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m760 \u001b[0m\u001b[2m│   │   │   \u001b[0mitems=train_items,                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m761 \u001b[0m\u001b[2m│   │   │   \u001b[0mkey=data_formation[\u001b[33m\"\u001b[0m\u001b[33mques_content\u001b[0m\u001b[33m\"\u001b[0m],                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m762 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrim_min_count=data_params.get(\u001b[33m\"\u001b[0m\u001b[33mtrim_min_count\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94m2\u001b[0m),                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/data/qlh/EduNLP/EduNLP/Pretrain/\u001b[0m\u001b[1;33mquesnet_vec.py\u001b[0m:\u001b[94m317\u001b[0m in \u001b[92mset_vocab\u001b[0m                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   │   \u001b[0msymbol=\u001b[94mNone\u001b[0m,                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   │   \u001b[0msilent=\u001b[94mTrue\u001b[0m,                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m316 \u001b[0m\u001b[2m│   \u001b[0m):                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m317 \u001b[2m│   │   \u001b[0mtoken_items = \u001b[96mself\u001b[0m.set_text_vocab(                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m318 \u001b[0m\u001b[2m│   │   │   \u001b[0mitems,                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m319 \u001b[0m\u001b[2m│   │   │   \u001b[0mkey=key,                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m320 \u001b[0m\u001b[2m│   │   │   \u001b[0mlower=lower,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/data/qlh/EduNLP/EduNLP/Pretrain/\u001b[0m\u001b[1;33mquesnet_vec.py\u001b[0m:\u001b[94m300\u001b[0m in \u001b[92mset_text_vocab\u001b[0m                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m297 \u001b[0m\u001b[2;33m│   │   │   \u001b[0m\u001b[33mthe lower bound number for adding a word into vocabulary, by default 1\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m298 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33msilent\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m299 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m300 \u001b[2m│   │   \u001b[0mtoken_items = \u001b[96mself\u001b[0m.tokenize(items, key=key, symbol=symbol) \u001b[94mif\u001b[0m do_tokenize \u001b[94melse\u001b[0m [   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m301 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.vocab.set_vocab(corpus_items=token_items, trim_min_count=trim_min_count, lo   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m302 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.stoi[\u001b[33m'\u001b[0m\u001b[33mword\u001b[0m\u001b[33m'\u001b[0m] = \u001b[96mself\u001b[0m.vocab.token_to_idx                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m303 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.itos[\u001b[33m'\u001b[0m\u001b[33mword\u001b[0m\u001b[33m'\u001b[0m] = \u001b[96mself\u001b[0m.vocab.idx_to_token                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/data/qlh/EduNLP/EduNLP/Pretrain/\u001b[0m\u001b[1;33mpretrian_utils.py\u001b[0m:\u001b[94m299\u001b[0m in \u001b[92mtokenize\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m296 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(items, \u001b[96mstr\u001b[0m) \u001b[95mor\u001b[0m \u001b[96misinstance\u001b[0m(items, \u001b[96mdict\u001b[0m):                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m297 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._tokenize(items, key=key, **kwargs)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m298 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m299 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m [\u001b[96mself\u001b[0m._tokenize(item, key=key, **kwargs) \u001b[94mfor\u001b[0m item \u001b[95min\u001b[0m items]             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m300 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m301 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mencode\u001b[0m(\u001b[96mself\u001b[0m, items: Tuple[\u001b[96mstr\u001b[0m, \u001b[96mdict\u001b[0m, List[\u001b[96mstr\u001b[0m], List[\u001b[96mdict\u001b[0m]], key=\u001b[94mlambda\u001b[0m x: x, **   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m302 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(items, \u001b[96mstr\u001b[0m) \u001b[95mor\u001b[0m \u001b[96misinstance\u001b[0m(items, \u001b[96mdict\u001b[0m):                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/data/qlh/EduNLP/EduNLP/Pretrain/\u001b[0m\u001b[1;33mpretrian_utils.py\u001b[0m:\u001b[94m299\u001b[0m in \u001b[92m<listcomp>\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m296 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(items, \u001b[96mstr\u001b[0m) \u001b[95mor\u001b[0m \u001b[96misinstance\u001b[0m(items, \u001b[96mdict\u001b[0m):                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m297 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._tokenize(items, key=key, **kwargs)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m298 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m299 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m [\u001b[96mself\u001b[0m._tokenize(item, key=key, **kwargs) \u001b[94mfor\u001b[0m item \u001b[95min\u001b[0m items]             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m300 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m301 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mencode\u001b[0m(\u001b[96mself\u001b[0m, items: Tuple[\u001b[96mstr\u001b[0m, \u001b[96mdict\u001b[0m, List[\u001b[96mstr\u001b[0m], List[\u001b[96mdict\u001b[0m]], key=\u001b[94mlambda\u001b[0m x: x, **   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m302 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(items, \u001b[96mstr\u001b[0m) \u001b[95mor\u001b[0m \u001b[96misinstance\u001b[0m(items, \u001b[96mdict\u001b[0m):                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/data/qlh/EduNLP/EduNLP/Pretrain/\u001b[0m\u001b[1;33mpretrian_utils.py\u001b[0m:\u001b[94m317\u001b[0m in \u001b[92m_tokenize\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mNotImplementedError\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m316 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_tokenize\u001b[0m(\u001b[96mself\u001b[0m, item: Tuple[\u001b[96mstr\u001b[0m, \u001b[96mdict\u001b[0m], key=\u001b[94mlambda\u001b[0m x: x, **argv):                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m317 \u001b[2m│   │   \u001b[0mtoken_item = \u001b[96mself\u001b[0m.text_tokenizer._tokenize(item, key=key, **argv)                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m318 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(token_item) == \u001b[94m0\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m319 \u001b[0m\u001b[2m│   │   │   \u001b[0mtoken_item = [\u001b[96mself\u001b[0m.vocab.unk_token]                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m320 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(token_item) > \u001b[96mself\u001b[0m.max_length:                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/data/qlh/EduNLP/EduNLP/Tokenizer/\u001b[0m\u001b[1;33mtokenizer.py\u001b[0m:\u001b[94m70\u001b[0m in \u001b[92m_tokenize\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 67 \u001b[0m\u001b[2;33m│   │   │   \u001b[0m\u001b[33mdetermine how to get the text of item, by default lambdax: x\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 68 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 69 \u001b[0m\u001b[2m│   │   \u001b[0msymbol = \u001b[96mself\u001b[0m.symbol \u001b[94mif\u001b[0m symbol \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m symbol                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 70 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m tokenize(seg(key(item), symbol=symbol, figures=\u001b[96mself\u001b[0m.figures),               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 71 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m**\u001b[96mself\u001b[0m.tokenization_params, **kwargs).tokens                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 72 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 73 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[32m'str'\u001b[0m object is not callable\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 自定义训练参数\n",
    "train_params = {\n",
    "    # train params\n",
    "    \"n_epochs\": 1,\n",
    "    \"batch_size\": 1,\n",
    "    \"lr\": 1e-3,\n",
    "    'save_every': 1,\n",
    "    'log_steps': 10,\n",
    "    # 'device': 'cpu',\n",
    "    'max_steps': 2,\n",
    "    # model params\n",
    "    'emb_size': 256,\n",
    "    'feat_size': 256,\n",
    "}\n",
    "\n",
    "# 当前仅支持linux下训练\n",
    "pretrain_quesnet(\n",
    "    raw_data,\n",
    "    output_dir,\n",
    "    # tokenizer=tokenizer,\n",
    "    img_dir=None,\n",
    "    save_embs=True,\n",
    "    load_embs=False,\n",
    "    train_params=train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_dir = os.path.join(output_dir, \"quesnet_test_256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 使用训练好的QuesNet Tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取保存的tokenizer\n",
    "tokenizer = QuesNetTokenizer.from_pretrained(pretrain_dir,\n",
    "                                             img_dir=os.path.join(data_dir, \"quesnet_img\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['已知', '集合', 'A', '=', '\\\\left', '\\\\{', 'x', '\\\\mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\right', '\\\\}', ',', '\\\\quad', 'B', '=', '\\\\{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\}', ',', '\\\\quad', 'A', '\\\\cap', 'B', '=']\n",
      "\n",
      "[['已知', '集合', 'A', '=', '\\\\left', '\\\\{', 'x', '\\\\mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\right', '\\\\}', ',', '\\\\quad', 'B', '=', '\\\\{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\}', ',', '\\\\quad', 'A', '\\\\cap', 'B', '='], ['复数', 'z', '=', '1', '+', '2', 'i', '+', 'i', '^', '{', '3', '}', '|', 'z', '|', '='], ['埃及', '胡夫', '金字塔', '古代', '世界', '建筑', '奇迹', '形状', '视为', '正四', '棱锥', '以该', '四', '棱锥', '高为', '边长', '正方形', '面积', '等于', '四', '棱锥', '侧面', '三角形', '面积', '侧面', '三角形', '底边', '高', '底面', '正方形', '边长', '比值'], ['设', 'O', '正方形', 'ABCD', '中心', 'O', ',', 'A', ',', 'B', ',', 'C', ',', 'D', '中任取', '3', '点', '取到', '3', '点', '共线', '概率'], ['某校', '课外', '学习', '小组', '研究', '作物', '发芽率', 'y', '温度', 'x', '单位', '^', '{', '\\\\circ', '}', '\\\\mathrm', '{', 'C', '}', '关系', '20', '温度', '条件', '种子', '发芽', '实验', '实验', '数据', '\\\\left', '(', 'x', '_', '{', 'i', '}', ',', 'y', '_', '{', 'i', '}', '\\\\right', ')', '(', 'i', '=', '1', ',', '2', ',', '\\\\cdots', ',', '20', ')', '散点图', \\FigureID{000004d6-0479-11ec-829b-797d5eb43535}, '散点图', '10', '^', '{', '\\\\circ', '}', '\\\\mathrm', '{', 'C', '}', '40', '^', '{', '\\\\circ', '}', '\\\\mathrm', '{', 'C', '}', '之间', '四个', '回归方程', '类型', '中', '适宜', '发芽率', 'y', '温度', 'x', '回归方程', '类型']]\n",
      "\n",
      "{'content_idx': [0, 0, 0, 14, 21, 0, 32, 0, 32, 27, 34, 10, 35, 7, 0, 32, 7, 0, 0, 0, 25, 0, 6, 0, 0, 14, 0, 7, 0, 6, 8, 6, 0, 6, 0, 0, 6, 0, 0, 0, 0, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'meta_idx': {'know_name': [0, 0, 0]}, 'content': ['已知', '集合', 'A', '=', '\\\\left', '\\\\{', 'x', '\\\\mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\right', '\\\\}', ',', '\\\\quad', 'B', '=', '\\\\{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\}', ',', '\\\\quad', 'A', '\\\\cap', 'B', '='], 'meta': {'know_name': ['代数', '集合', '集合的相等']}}\n",
      "\n",
      "{'content_idx': [[0, 0, 0, 14, 21, 0, 32, 0, 32, 27, 34, 10, 35, 7, 0, 32, 7, 0, 0, 0, 25, 0, 6, 0, 0, 14, 0, 7, 0, 6, 8, 6, 0, 6, 0, 0, 6, 0, 0, 0, 0, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 14, 8, 5, 10, 30, 5, 30, 27, 34, 0, 35, 0, 0, 0, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [56, 81, 88, 48, 37, 63, 57, 64, 82, 72, 71, 40, 51, 71, 91, 86, 73, 89, 79, 51, 71, 42, 36, 89, 42, 36, 61, 90, 62, 73, 86, 74, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], 'meta_idx': [{'know_name': [0, 0, 0]}, {'know_name': [0, 0, 0]}, {'know_name': [8, 7, 5]}]}\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "# 可以处理单个题目\n",
    "print(tokenizer.tokenize(raw_data[0], key=lambda x: x['ques_content']))\n",
    "print()\n",
    "# 也可以处理题目列表\n",
    "print(tokenizer.tokenize(raw_data[:5], key=lambda x: x['ques_content']))\n",
    "\n",
    "print()\n",
    "\n",
    "# 将token转换为index\n",
    "print(tokenizer(raw_data[0], key=lambda x: x['ques_content'], return_text=True, padding=True))\n",
    "print()\n",
    "print(tokenizer(raw_data[:3], key=lambda x: x['ques_content'], padding=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 使用训练好的QuesNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    'tokenizer_config_dir': pretrain_dir,\n",
    "}\n",
    "i2v = QuesNet('quesnet', 'quesnet', pretrain_dir,\n",
    "              tokenizer_kwargs=tokenizer_kwargs, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n",
      "torch.Size([1, 43, 256])\n",
      "\n",
      "torch.Size([1, 43, 256])\n",
      "torch.Size([1, 256])\n",
      "\n",
      "torch.Size([2, 43, 256])\n",
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# 获得单个题目的表征\n",
    "i_vec, t_vec = i2v(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "print(i_vec.shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 也可以分别获得题目表征和各个token的表征\n",
    "t_vec = i2v.infer_token_vector(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "i_vec = i2v.infer_item_vector(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "print(t_vec.shape)\n",
    "print(i_vec.shape)\n",
    "print()\n",
    "\n",
    "# 获得题目列表的表征\n",
    "t_vec = i2v.infer_token_vector(raw_data[:2], key=lambda x: x[\"ques_content\"])\n",
    "i_vec = i2v.infer_item_vector(raw_data[:2], key=lambda x: x[\"ques_content\"])\n",
    "print(t_vec.shape)\n",
    "print(i_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 使用EduNLP中公开的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EduNLP, INFO model_path: ..\\..\\examples\\test_model\\quesnet\\quesnet_test_256\n",
      "EduNLP, INFO Use pretrained t2v model quesnet_test_256\n",
      "downloader, INFO http://base.ustc.edu.cn/data/model_zoo/modelhub/quesnet_pub/1/quesnet_test_256.zip is saved as ..\\..\\examples\\test_model\\quesnet\\quesnet_test_256.zip\n",
      "downloader, INFO file existed, skipped\n"
     ]
    }
   ],
   "source": [
    "# 获取公开的预训练模型\n",
    "i2v = get_pretrained_i2v(\"quesnet_test_256\", model_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n",
      "torch.Size([1, 43, 256])\n",
      "\n",
      "torch.Size([1, 43, 256])\n",
      "torch.Size([1, 256])\n",
      "\n",
      "torch.Size([2, 43, 256])\n",
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# 用法和I2V相同\n",
    "\n",
    "# 获得单个题目的表征\n",
    "i_vec, t_vec = i2v(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "print(i_vec.shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 也可以分别获得题目表征和各个token的表征\n",
    "t_vec = i2v.infer_token_vector(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "i_vec = i2v.infer_item_vector(raw_data[0], key=lambda x: x[\"ques_content\"])\n",
    "print(t_vec.shape)\n",
    "print(i_vec.shape)\n",
    "print()\n",
    "\n",
    "# 获得题目列表的表征\n",
    "t_vec = i2v.infer_token_vector(raw_data[:2], key=lambda x: x[\"ques_content\"])\n",
    "i_vec = i2v.infer_item_vector(raw_data[:2], key=lambda x: x[\"ques_content\"])\n",
    "print(t_vec.shape)\n",
    "print(i_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
