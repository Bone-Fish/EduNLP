{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qlh/anaconda3/envs/dev/lib/python3.6/site-packages/google/auth/crypt/_cryptography_rsa.py:22: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  import cryptography.exceptions\n",
      "/home/qlh/anaconda3/envs/dev/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from EduNLP.Pretrain import PretrainedEduTokenizer, EduDataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "BASE_DIR = \"../..\"\n",
    "data_dir = f\"{BASE_DIR}/static/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/data/pretrain_test_models/pretrain/\"\n",
    "\n",
    "\n",
    "def stem_data():\n",
    "    _data = []\n",
    "    data_path = os.path.join(data_dir, \"standard_luna_data.json\")\n",
    "    with open(data_path, encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            _data.append(json.loads(line))\n",
    "    return _data\n",
    "\n",
    "train_items = stem_data()\n",
    "\n",
    "test_items = [\n",
    "    {'ques_content': '有公式$\\\\FormFigureID{wrong1?}$和公式$\\\\FormFigureBase64{wrong2?}$，\\\n",
    "            如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$,\\\n",
    "            若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'},\n",
    "    {'ques_content': '如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$, \\\n",
    "            若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PretrainedEduTokenizer\n",
    "\n",
    "该类主要用于处理预训练模型的输入语料，主要成分包括词表(vocab) 和 基础令牌话容器，负责将输入语料处理为适合模型的输入格式。\n",
    "\n",
    "## 1.1 构造令牌化容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "corpus_items = train_items + test_items\n",
    "\n",
    "# 定义参数\n",
    "tokenizer_params = {\n",
    "    \"add_specials\": True,\n",
    "    \"tokenize_method\": \"pure_text\",\n",
    "}\n",
    "\n",
    "tokenizer = PretrainedEduTokenizer(**tokenizer_params)\n",
    "print(len(tokenizer))\n",
    "\n",
    "\n",
    "# 设置预训练语料，训练令牌话容器\n",
    "tokenizer.set_vocab(corpus_items, key=lambda x: x['ques_content'])\n",
    "print(len(tokenizer))\n",
    "\n",
    "# 保存令牌话容器\n",
    "pretrained_tokenizer_dir = output_dir\n",
    "tokenizer.save_pretrained(pretrained_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 使用令牌化容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seq_idx', 'seq_len']\n",
      "torch.Size([2, 17])\n",
      "\n",
      "['seq_idx', 'seq_len']\n",
      "torch.Size([2, 100])\n",
      "\n",
      "[[305, 305, 238, 6, 20, 33, 86, 166, 9, 40, 17, 20, 41, 140, 86, 175, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [238, 6, 20, 33, 86, 166, 9, 40, 17, 20, 41, 140, 86, 175, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "['seq_idx', 'seq_len', 'seq_token']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载令牌话容器\n",
    "tokenizer = PretrainedEduTokenizer.from_pretrained(pretrained_tokenizer_dir)\n",
    "\n",
    "# 按batch进行padding\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'])\n",
    "print(list(encodes.keys()))\n",
    "print(encodes[\"seq_idx\"].shape)\n",
    "print()\n",
    "\n",
    "# 按max_length进行padding\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'], padding=\"max_length\", max_length=100)\n",
    "print(list(encodes.keys()))\n",
    "print(encodes[\"seq_idx\"].shape)\n",
    "print()\n",
    "\n",
    "# 不返回tensor\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'], padding=\"max_length\", max_length=100, return_tensors=False)\n",
    "print(encodes[\"seq_idx\"])\n",
    "print()\n",
    "\n",
    "# 保留tokens\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'], padding=\"max_length\", max_length=100, return_text=True)\n",
    "print(list(encodes.keys()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 其他操作\n",
    "\n",
    "扩充词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码/解码 句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改基础令牌化容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EduDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "393c785ae146746c01a0a4916492280e86e054d5dc7bd29abe47f41733a5976b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
