{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from EduNLP.ModelZoo.rnn import ElmoLM\n",
    "from EduNLP.Pretrain import train_elmo, ElmoTokenizer\n",
    "from EduNLP.Vector import ElmoModel, T2V\n",
    "from EduNLP.I2V import Elmo, get_pretrained_i2v\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练自己的Elmo模型\n",
    "## 1. 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置你的数据路径和输出路径\n",
    "BASE_DIR = \"../..\"\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/static/test_data\"\n",
    "output_dir = f\"{BASE_DIR}/data/pretrain_test_models/elmo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_data():\n",
    "    _data = []\n",
    "    data_path = os.path.join(data_dir, \"standard_luna_data.json\")\n",
    "    with open(data_path, encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            _data.append(json.loads(line))\n",
    "    return _data\n",
    "\n",
    "train_items = stem_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1d2664c1af48f7b6d16b5a5fbe4c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config PretrainedConfig {\n",
      "  \"architecture\": \"ElmoLM\",\n",
      "  \"batch_first\": true,\n",
      "  \"dropout_rate\": 0.5,\n",
      "  \"embedding_dim\": 300,\n",
      "  \"hidden_size\": 300,\n",
      "  \"num_layers\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_pack_pad\": false,\n",
      "  \"vocab_size\": 305\n",
      "}\n",
      "\n",
      "Model config PretrainedConfig {\n",
      "  \"architecture\": \"ElmoLMForPreTraining\",\n",
      "  \"batch_first\": true,\n",
      "  \"dropout_rate\": 0.5,\n",
      "  \"embedding_dim\": 300,\n",
      "  \"hidden_size\": 300,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_pack_pad\": false,\n",
      "  \"vocab_size\": 305\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/qlh/anaconda3/envs/dev/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 25\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2\n",
      "/home/qlh/anaconda3/envs/dev/lib/python3.6/site-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "/home/qlh/anaconda3/envs/dev/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../../data/pretrain_test_models/elmo/\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Configuration saved in ../../data/pretrain_test_models/elmo/config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../data/pretrain_test_models/elmo/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义训练参数\n",
    "train_params = {\n",
    "  # \"emb_dim\": 128,\n",
    "  # \"hid_dim\": 256,\n",
    "  # \"batch_size\": 4,\n",
    "  # \"epochs\": 1,\n",
    "  # \"lr\": 5e-3,\n",
    "  # \"device\": None,\n",
    "  \n",
    "  \"num_train_epochs\": 1,\n",
    "  \"per_device_train_batch_size\": 8,\n",
    "  \"save_steps\": 50,\n",
    "  \"save_total_limit\": 2,\n",
    "  \"logging_steps\": 5,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"learning_rate\": 5e-4,\n",
    "}\n",
    "\n",
    "train_elmo(train_items, output_dir, train_params=train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = [\n",
    "    {'ques_content': '有公式$\\\\FormFigureID{wrong1?}$和公式$\\\\FormFigureBase64{wrong2?}$，\\\n",
    "            如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$,\\\n",
    "            若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'},\n",
    "    {'ques_content': '如图$\\\\FigureID{088f15ea-8b7c-11eb-897e-b46bfc50aa29}$, \\\n",
    "            若$x,y$满足约束条件$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 直接加载令牌容器和模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config PretrainedConfig {\n",
      "  \"architecture\": \"ElmoLM\",\n",
      "  \"batch_first\": true,\n",
      "  \"dropout_rate\": 0.5,\n",
      "  \"embedding_dim\": 300,\n",
      "  \"hidden_size\": 300,\n",
      "  \"num_layers\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_pack_pad\": false,\n",
      "  \"vocab_size\": 305\n",
      "}\n",
      "\n",
      "[EduNLP, INFO] All the weights of ElmoLM were initialized from the model checkpoint at ../../data/pretrain_test_models/elmo/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElmoLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElmoLMOutput([('pred_forward',\n",
       "               tensor([[[0.0033, 0.0032, 0.0034,  ..., 0.0033, 0.0031, 0.0035],\n",
       "                        [0.0032, 0.0032, 0.0035,  ..., 0.0033, 0.0031, 0.0033],\n",
       "                        [0.0035, 0.0031, 0.0034,  ..., 0.0032, 0.0033, 0.0033],\n",
       "                        ...,\n",
       "                        [0.0032, 0.0032, 0.0036,  ..., 0.0030, 0.0031, 0.0033],\n",
       "                        [0.0034, 0.0032, 0.0035,  ..., 0.0032, 0.0031, 0.0035],\n",
       "                        [0.0034, 0.0031, 0.0032,  ..., 0.0033, 0.0031, 0.0033]],\n",
       "               \n",
       "                       [[0.0034, 0.0030, 0.0034,  ..., 0.0033, 0.0032, 0.0034],\n",
       "                        [0.0035, 0.0031, 0.0037,  ..., 0.0031, 0.0031, 0.0035],\n",
       "                        [0.0035, 0.0030, 0.0034,  ..., 0.0031, 0.0033, 0.0035],\n",
       "                        ...,\n",
       "                        [0.0032, 0.0032, 0.0032,  ..., 0.0032, 0.0032, 0.0034],\n",
       "                        [0.0034, 0.0030, 0.0033,  ..., 0.0033, 0.0030, 0.0033],\n",
       "                        [0.0035, 0.0032, 0.0032,  ..., 0.0032, 0.0030, 0.0032]]],\n",
       "                      grad_fn=<SoftmaxBackward>)),\n",
       "              ('pred_backward',\n",
       "               tensor([[[0.0032, 0.0029, 0.0033,  ..., 0.0030, 0.0028, 0.0033],\n",
       "                        [0.0032, 0.0031, 0.0034,  ..., 0.0031, 0.0029, 0.0033],\n",
       "                        [0.0032, 0.0031, 0.0034,  ..., 0.0031, 0.0030, 0.0035],\n",
       "                        ...,\n",
       "                        [0.0031, 0.0032, 0.0034,  ..., 0.0032, 0.0031, 0.0034],\n",
       "                        [0.0034, 0.0033, 0.0035,  ..., 0.0031, 0.0030, 0.0034],\n",
       "                        [0.0034, 0.0032, 0.0036,  ..., 0.0033, 0.0030, 0.0035]],\n",
       "               \n",
       "                       [[0.0033, 0.0032, 0.0036,  ..., 0.0031, 0.0029, 0.0033],\n",
       "                        [0.0033, 0.0031, 0.0035,  ..., 0.0031, 0.0031, 0.0033],\n",
       "                        [0.0034, 0.0031, 0.0033,  ..., 0.0030, 0.0031, 0.0032],\n",
       "                        ...,\n",
       "                        [0.0033, 0.0032, 0.0034,  ..., 0.0030, 0.0031, 0.0034],\n",
       "                        [0.0033, 0.0034, 0.0032,  ..., 0.0032, 0.0030, 0.0033],\n",
       "                        [0.0034, 0.0033, 0.0033,  ..., 0.0032, 0.0032, 0.0033]]],\n",
       "                      grad_fn=<SoftmaxBackward>)),\n",
       "              ('forward_output',\n",
       "               tensor([[[-0.0528, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.1094],\n",
       "                        [-0.0555, -0.0000,  0.1578,  ..., -0.1109, -0.0000, -0.1539],\n",
       "                        [-0.0000, -0.0000,  0.1170,  ..., -0.1780, -0.0090, -0.0000],\n",
       "                        ...,\n",
       "                        [-0.0000, -0.0493,  0.0206,  ...,  0.0145, -0.0501, -0.0000],\n",
       "                        [-0.0000, -0.0000, -0.0088,  ..., -0.0000, -0.0375,  0.0128],\n",
       "                        [-0.0412, -0.1187,  0.0000,  ..., -0.0000,  0.0000,  0.0386]],\n",
       "               \n",
       "                       [[ 0.0173, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "                        [ 0.0000, -0.1261, -0.0141,  ..., -0.0000,  0.0211,  0.0752],\n",
       "                        [ 0.0000, -0.1159, -0.0309,  ..., -0.1112, -0.0282,  0.0501],\n",
       "                        ...,\n",
       "                        [-0.0000, -0.1322,  0.0000,  ..., -0.0242,  0.0000,  0.0000],\n",
       "                        [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0492,  0.0000],\n",
       "                        [ 0.0000, -0.2027,  0.1891,  ...,  0.0292,  0.0457, -0.0000]]],\n",
       "                      grad_fn=<MulBackward0>)),\n",
       "              ('backward_output',\n",
       "               tensor([[[ 0.1090, -0.1446,  0.0000,  ..., -0.0652,  0.0701, -0.0444],\n",
       "                        [ 0.0911, -0.1078,  0.0514,  ..., -0.0000,  0.0735,  0.0000],\n",
       "                        [ 0.0000, -0.0000,  0.0000,  ..., -0.0463,  0.0000,  0.0000],\n",
       "                        ...,\n",
       "                        [-0.0984, -0.0927,  0.1122,  ...,  0.0556,  0.0000, -0.0028],\n",
       "                        [-0.0000, -0.0939,  0.0403,  ...,  0.0000,  0.0629, -0.0146],\n",
       "                        [-0.0000, -0.0779,  0.0000,  ..., -0.0000,  0.0000, -0.0187]],\n",
       "               \n",
       "                       [[-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0062,  0.0000],\n",
       "                        [-0.0000, -0.0482,  0.0000,  ..., -0.0000, -0.0000,  0.0713],\n",
       "                        [-0.0000, -0.1221,  0.0000,  ..., -0.0000, -0.0000,  0.1571],\n",
       "                        ...,\n",
       "                        [ 0.0640, -0.1978,  0.0387,  ..., -0.0000,  0.1457, -0.0000],\n",
       "                        [ 0.1365, -0.2237,  0.0940,  ..., -0.1624,  0.0000, -0.0368],\n",
       "                        [ 0.1113, -0.0000,  0.0000,  ..., -0.1166,  0.0000, -0.0000]]],\n",
       "                      grad_fn=<MulBackward0>))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_dir = output_dir\n",
    "\n",
    "model = ElmoLM.from_pretrained(pretrained_model_dir)\n",
    "tokenizer = ElmoTokenizer.from_pretrained(pretrained_model_dir)\n",
    "\n",
    "encodes = tokenizer(test_items, lambda x: x['ques_content'])\n",
    "model(**encodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用I2V向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config PretrainedConfig {\n",
      "  \"architecture\": \"ElmoLM\",\n",
      "  \"batch_first\": true,\n",
      "  \"dropout_rate\": 0.5,\n",
      "  \"embedding_dim\": 300,\n",
      "  \"hidden_size\": 300,\n",
      "  \"num_layers\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_pack_pad\": false,\n",
      "  \"vocab_size\": 305\n",
      "}\n",
      "\n",
      "[EduNLP, INFO] All the weights of ElmoLM were initialized from the model checkpoint at ../../data/pretrain_test_models/elmo/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElmoLM for predictions without further training.\n",
      "/home/qlh/EduNLP/EduNLP/Vector/elmo_vec.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (outputs.forward_output[torch.arange(len(items[\"seq_len\"])), torch.tensor(items[\"seq_len\"]) - 1],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 600])\n",
      "torch.Size([1, 15, 600])\n",
      "torch.Size([2, 600])\n",
      "torch.Size([2, 25, 600])\n"
     ]
    }
   ],
   "source": [
    "tokenizer_kwargs = {\"tokenizer_config_dir\": pretrained_model_dir}\n",
    "i2v = Elmo('elmo', 'elmo', output_dir, tokenizer_kwargs=tokenizer_kwargs)\n",
    "\n",
    "# 可以对单个题目进行表征\n",
    "i_vec, t_vec = i2v(test_items[0], key=lambda x: x[\"ques_content\"])\n",
    "print(i_vec.shape) # == torch.Size([x])\n",
    "print(t_vec.shape) # == torch.Size([x, x])\n",
    "\n",
    "# 也可以对题目列表进行表征\n",
    "i_vec, t_vec = i2v(test_items, key=lambda x: x[\"ques_content\"])\n",
    "print(i_vec.shape) # == torch.Size([2, x])\n",
    "print(t_vec.shape) # == torch.Size([2, x, x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 使用Tokenizer和T2V向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config PretrainedConfig {\n",
      "  \"architecture\": \"ElmoLM\",\n",
      "  \"batch_first\": true,\n",
      "  \"dropout_rate\": 0.5,\n",
      "  \"embedding_dim\": 300,\n",
      "  \"hidden_size\": 300,\n",
      "  \"num_layers\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_pack_pad\": false,\n",
      "  \"vocab_size\": 305\n",
      "}\n",
      "\n",
      "[EduNLP, INFO] All the weights of ElmoLM were initialized from the model checkpoint at ../../data/pretrain_test_models/elmo/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElmoLM for predictions without further training.\n",
      "/home/qlh/EduNLP/EduNLP/Vector/elmo_vec.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (outputs.forward_output[torch.arange(len(items[\"seq_len\"])), torch.tensor(items[\"seq_len\"]) - 1],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 600])\n",
      "\n",
      "torch.Size([2, 600])\n",
      "torch.Size([2, 25, 600])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载之前训练的模型tokenizer\n",
    "tokenizer = ElmoTokenizer.from_pretrained(pretrained_model_dir)\n",
    "encodes = tokenizer(test_items, key=lambda x: x['ques_content'])\n",
    "\n",
    "t2v = ElmoModel(pretrained_model_dir)\n",
    "\n",
    "i_vec = t2v(encodes)\n",
    "print(i_vec.shape) # == torch.Size([2, x])\n",
    "print()\n",
    "\n",
    "i_vec = t2v.infer_vector(encodes)\n",
    "t_vec = t2v.infer_tokens(encodes)\n",
    "print(i_vec.shape) # == torch.Size([2, x])\n",
    "print(t_vec.shape) # == torch.Size([2, x, x]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 使用EduNLP中公开的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取公开的预训练模型\n",
    "pretrained_dir = f\"{BASE_DIR}/examples/test_model/elmo\"\n",
    "i2v = get_pretrained_i2v(\"elmo_test\", model_dir=pretrained_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_vec, t_vec = i2v(test_items)\n",
    "print(i_vec.shape)\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 也可以单独获取题目表征和各个token的表征\n",
    "i_vec = i2v.infer_item_vector(test_items, key=lambda x: x['ques_content'])\n",
    "print(i_vec.shape)\n",
    "t_vec = i2v.infer_token_vector(test_items, key=lambda x: x['ques_content'])\n",
    "print(t_vec.shape)\n",
    "print()\n",
    "\n",
    "# 同样，可以获取单个题目的表征\n",
    "i_vec, t_vec = i2v(test_items[0], key=lambda x: x['ques_content'])\n",
    "print(i_vec.shape)\n",
    "print(t_vec.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "393c785ae146746c01a0a4916492280e86e054d5dc7bd29abe47f41733a5976b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
