{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "edfef487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EduNLP.Tokenizer import CharTokenizer, SpaceTokenizer, CustomTokenizer, PureTextTokenizer, AstFormulaTokenizer, get_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d344c",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "The basic tokenization containers currently available include:\n",
    "\n",
    "- CharTokenizer\n",
    "- SpaceTokenizer\n",
    "- CustomTokenizer\n",
    "- PureTextTokenizer\n",
    "- AstFormulaTokenizer\n",
    "\n",
    "Here is the detailed introduction for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71450b8a",
   "metadata": {},
   "source": [
    "## CustomTokenizer\n",
    "\n",
    "- `custom` Tokenizer can tokenize SIF items by customized configuration. The supported custom content includes: pre-tokenized content, image attribute information, etc. You can specify the stop words in order to filter a subset of content\n",
    "\n",
    "- This tokenizer works almost the same as the pure text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "2ec7f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文具店', '[FORMULA]', '练习本', '卖出', '剩', '[FORMULA]', '包', '每包', '[FORMULA]', '卖出']\n"
     ]
    }
   ],
   "source": [
    "items = [{\n",
    "        \"stem\": \"文具店有 $600$ 本练习本，卖出一些后，还剩 $4$ 包，每包 $25$ 本，卖出多少本？\",\n",
    "        \"options\": [\"1\", \"2\"]\n",
    "        }]\n",
    "tokenizer = get_tokenizer(\"custom\", symbol='f')\n",
    "\n",
    "tokens = tokenizer(items, key = lambda x: x['stem'])\n",
    "print(next(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "257f6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[TEXT]', 'A', '=', '\\\\left', '\\\\{', 'x', '\\\\mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\right', '\\\\}', ',', '\\\\quad', 'B', '=', '\\\\{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\}', ',', '\\\\quad', '[TEXT]', 'A', '\\\\cap', 'B', '=']\n"
     ]
    }
   ],
   "source": [
    "items = [{\n",
    "        \"stem\": \"已知集合$A=\\\\left\\\\{x \\\\mid x^{2}-3 x-4<0\\\\right\\\\}, \\\\quad B=\\\\{-4,1,3,5\\\\}, \\\\quad$ 则 $A \\\\cap B=$\",\n",
    "        \"options\": [\"1\", \"2\"]\n",
    "        }]\n",
    "\n",
    "tokenizer = get_tokenizer(\"custom\", symbol='t')\n",
    "\n",
    "tokens = tokenizer(items, key=lambda x: x[\"stem\"])\n",
    "print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78cdae1",
   "metadata": {},
   "source": [
    "## CharTokenizer\n",
    "\n",
    "- `char` Tokenizer extracts each word of the text individually as a separate token. You can specify the stop words in order to filter a subset of content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "ab08e850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文', '具', '店', '有', '$', '600', '$', '本', '练', '习', '本', '卖', '出', '一', '些', '后', '还', '剩', '$', '4', '$', '包', '每', '包', '$', '25', '$', '本', '卖', '出', '多', '少', '本']\n"
     ]
    }
   ],
   "source": [
    "items = [{\n",
    "        \"stem\": \"文具店有 $600$ 本练习本，卖出一些后，还剩 $4$ 包，每包 $25$ 本，卖出多少本？\",\n",
    "        \"options\": [\"1\", \"2\"]\n",
    "        }]\n",
    "tokenizer = get_tokenizer(\"char\", stop_words = set(\"，？\"))\n",
    "\n",
    "tokens = tokenizer(items, key = lambda x: x['stem'])\n",
    "print(next(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "0cf5f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['已', '知', '集', '合', '$', 'A', '=', '\\\\', 'left', '\\\\', '{', 'x', '\\\\', 'mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\', 'right', '\\\\', '}', ',', '\\\\', 'quad', 'B', '=', '\\\\', '{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\', '}', ',', '\\\\', 'quad', '$', '则', '$', 'A', '\\\\', 'cap', 'B', '=', '$']\n"
     ]
    }
   ],
   "source": [
    "items = [{\n",
    "        \"stem\": \"已知集合$A=\\\\left\\\\{x \\\\mid x^{2}-3 x-4<0\\\\right\\\\}, \\\\quad B=\\\\{-4,1,3,5\\\\}, \\\\quad$ 则 $A \\\\cap B=$\",\n",
    "        \"options\": [\"1\", \"2\"]\n",
    "        }]\n",
    "\n",
    "tokenizer = get_tokenizer(\"char\")\n",
    "tokens = tokenizer(items, key=lambda x: x[\"stem\"])\n",
    "print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132f7e4",
   "metadata": {},
   "source": [
    "## SpaceTokenizer\n",
    "\n",
    "- `space` Tokenizer splits the text based on the space. You can specify the stop words in order to filter a subset of content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "36c32fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文具店有', '$600$', '本练习本，卖出一些后，还剩', '$4$', '包，每包', '$25$', '本，卖出多少本？']\n"
     ]
    }
   ],
   "source": [
    "items = ['文具店有 $600$ 本练习本，卖出一些后，还剩 $4$ 包，每包 $25$ 本，卖出多少本？']\n",
    "\n",
    "tokenizer = get_tokenizer(\"space\", stop_words = [])\n",
    "tokens= tokenizer(items)\n",
    "\n",
    "print(next(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "fc62535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['已知集合$A=\\\\left\\\\{x', '\\\\mid', 'x^{2}-3', 'x-4<0\\\\right\\\\},', '\\\\quad', 'B=\\\\{-4,1,3,5\\\\},', '\\\\quad$', '则', '$A', '\\\\cap', 'B=$']\n"
     ]
    }
   ],
   "source": [
    "items = [{\n",
    "        \"stem\": \"已知集合$A=\\\\left\\\\{x \\\\mid x^{2}-3 x-4<0\\\\right\\\\}, \\\\quad B=\\\\{-4,1,3,5\\\\}, \\\\quad$ 则 $A \\\\cap B=$\",\n",
    "        \"options\": [\"1\", \"2\"]\n",
    "        }]\n",
    "\n",
    "tokenizer = get_tokenizer(\"space\", stop_words = [])\n",
    "tokens = tokenizer(items, key=lambda x: x[\"stem\"])\n",
    "print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a987ba8",
   "metadata": {},
   "source": [
    "## PureTextTokenizer\n",
    "- `pure_text` Tokenizer treats all the elements in SIF item as prue text. Spectially, it will tokenize formulas as text. Besides, excess content can be discarded. The tokens will be a collection of simple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "b44452c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文具店', '600', '练习本', '卖出', '剩', '4', '包', '每包', '25', '卖出']\n"
     ]
    }
   ],
   "source": [
    "items = ['文具店有 $600$ 本练习本，卖出一些后，还剩 $4$ 包，每包 $25$ 本，卖出多少本？']\n",
    "\n",
    "tokenizer = get_tokenizer(\"pure_text\", stop_words = [])\n",
    "tokens= tokenizer(items)\n",
    "\n",
    "print(next(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "a25f8c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['已知', '集合', 'A', '=', '\\\\left', '\\\\{', 'x', '\\\\mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\right', '\\\\}', ',', '\\\\quad', 'B', '=', '\\\\{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\}', ',', '\\\\quad', 'A', '\\\\cap', 'B', '=']\n"
     ]
    }
   ],
   "source": [
    "items = [{\n",
    "        \"stem\": \"已知集合$A=\\\\left\\\\{x \\\\mid x^{2}-3 x-4<0\\\\right\\\\}, \\\\quad B=\\\\{-4,1,3,5\\\\}, \\\\quad$ 则 $A \\\\cap B=$\",\n",
    "        \"options\": [\"1\", \"2\"]\n",
    "        }]\n",
    "\n",
    "tokenizer = get_tokenizer(\"pure_text\", stop_words = [])\n",
    "tokens = tokenizer(items, key=lambda x: x[\"stem\"])\n",
    "print(next(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "a8f7e65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['公式', '如图', '[FIGURE]', 'x', ',', 'y', '约束条件', '公式', '[SEP]', 'z', '=', 'x', '+', '7', 'y', '最大值', '[MARK]']\n"
     ]
    }
   ],
   "source": [
    "items = [\"有公式$\\\\FormFigureID{1}$，如图$\\\\FigureID{088f15ea-xxx}$,若$x,y$满足约束条件公式$\\\\FormFigureBase64{2}$,$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$\"]\n",
    "\n",
    "tokenizer = get_tokenizer(\"pure_text\") \n",
    "tokens = tokenizer(items)\n",
    "print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f488f8",
   "metadata": {},
   "source": [
    "## AstFormulaTokenizer\n",
    "\n",
    "- `ast_formula` Tokenizer abstracts the mathematical formulas in text. For example, variables that appear will be recorded and tokenized in turn, and objects such as expressions and pictures will be preprocessed as tokens. You can specify the stop words in order to filter a subset of content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "22aeab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文具店', 'textord', 'textord', 'textord', '练习本', '卖出', '剩', 'textord', '包', '每包', 'textord', 'textord', '卖出']\n"
     ]
    }
   ],
   "source": [
    "items = ['文具店有 $600$ 本练习本，卖出一些后，还剩 $4$ 包，每包 $25$ 本，卖出多少本？']\n",
    "\n",
    "tokenizer = get_tokenizer(\"ast_formula\")\n",
    "tokens= tokenizer(items)\n",
    "\n",
    "print(next(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "f577f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['已知', '集合', 'mathord_0', '=', 'mathord_1', '\\\\mid', 'mathord_1', 'textord', '{ }', '\\\\supsub', '-', 'textord', 'mathord_1', '-', 'textord', '<', 'textord', '\\\\{', ',', 'mathord_2', '=', '\\\\{', '-', 'textord', ',', 'textord', ',', 'textord', ',', 'textord', '\\\\}', ',', 'mathord_0', '\\\\cap', 'mathord_2', '=']\n"
     ]
    }
   ],
   "source": [
    "items = [{\n",
    "        \"stem\": \"已知集合$A=\\\\left\\\\{x \\\\mid x^{2}-3 x-4<0\\\\right\\\\}, \\\\quad B=\\\\{-4,1,3,5\\\\}, \\\\quad$ 则 $A \\\\cap B=$\",\n",
    "        \"options\": [\"1\", \"2\"]\n",
    "        }]\n",
    "\n",
    "tokenizer = get_tokenizer(\"ast_formula\")\n",
    "tokens = tokenizer(items, key=lambda x: x[\"stem\"])\n",
    "print(next(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "5b78d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['公式', '[FORMULA]', '如图', '[FIGURE]', 'mathord_0', ',', 'mathord_1', '约束条件', '公式', '[FORMULA]', '[SEP]', 'mathord_2', '=', 'mathord_0', '+', 'textord', 'mathord_1', '最大值', '[MARK]']\n"
     ]
    }
   ],
   "source": [
    "items = [\"有公式$\\\\FormFigureID{1}$，如图$\\\\FigureID{088f15ea-xxx}$,若$x,y$满足约束条件公式$\\\\FormFigureBase64{2}$,$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$\"]\n",
    "\n",
    "tokenizer = get_tokenizer(\"ast_formula\") \n",
    "tokens = tokenizer(items)\n",
    "print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3e920",
   "metadata": {},
   "source": [
    "# GensimWordTokenizer and GensimSegTokenizer\n",
    "\n",
    "- GensimWordTokenizer is the standart basic Tokenizer for SIF items\n",
    "\n",
    "- GensimSegTokenizer is the standart basic Tokenizer for SIF items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "fd96e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EduNLP.Pretrain import GensimWordTokenizer, GensimSegTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a6dcd",
   "metadata": {},
   "source": [
    "## GensimWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "6f090e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['已知', '公式', \\FormFigureID{1}, '如图', '[FIGURE]', 'mathord', ',', 'mathord', '约束条件', '公式', [FORMULA], '[SEP]', 'mathord', '=', 'mathord', '+', 'textord', 'mathord', '最大值', '[MARK]']\n",
      "\n",
      "['已知', '公式', '[FORMULA]', '如图', '[FIGURE]', 'x', ',', 'y', '约束条件', '公式', '[FORMULA]', '[SEP]', 'z', '=', 'x', '+', '7', 'y', '最大值', '[MARK]']\n"
     ]
    }
   ],
   "source": [
    "item = \"已知有公式$\\\\FormFigureID{1}$，如图$\\\\FigureID{088f15ea-xxx}$, 若$x,y$满足约束条件公式$\\\\FormFigureBase64{2}$,$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$\"\n",
    "\n",
    "tokenizer = GensimWordTokenizer(symbol=\"gmas\")\n",
    "token_item = tokenizer(item)\n",
    "print(token_item.tokens)\n",
    "print()\n",
    "\n",
    "tokenizer = GensimWordTokenizer(symbol=\"gmas\", general=True)\n",
    "token_item = tokenizer(item)\n",
    "print(token_item.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7dca7",
   "metadata": {},
   "source": [
    "## GensimSegTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "6c222cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [['已知', '公式'], [\\FormFigureID{1}], ['如图'], ['[FIGURE]'], ['mathord', ',', 'mathord'], ['约束条件', '公式'], [[FORMULA]], ['mathord', '=', 'mathord', '+', 'textord', 'mathord'], ['最大值'], ['[MARK]']]\n",
      "\n",
      "19 ['已知', '公式', \\FormFigureID{1}, '如图', '[FIGURE]', 'mathord', ',', 'mathord', '约束条件', '公式', [FORMULA], 'mathord', '=', 'mathord', '+', 'textord', 'mathord', '最大值', '[MARK]']\n",
      "\n",
      "5 [['[TEXT_BEGIN]', '已知', '公式', '[FORMULA_BEGIN]', \\FormFigureID{1}, '[TEXT_BEGIN]', '如图', '[FIGURE]', '[TEXT_BEGIN]', '[FORMULA_BEGIN]', 'mathord', ',', 'mathord', '[TEXT_BEGIN]', '约束条件', '公式', '[FORMULA_BEGIN]', [FORMULA], '[TEXT_BEGIN]', '[SEP]'], ['[TEXT_BEGIN]'], ['[FORMULA_BEGIN]', 'mathord', '=', 'mathord', '+', 'textord', 'mathord'], ['[TEXT_BEGIN]', '最大值'], ['[MARK]']]\n",
      "\n",
      "5 [['[TEXT_BEGIN]', '已知', '公式', '[TEXT_END]', '[FORMULA_BEGIN]', \\FormFigureID{1}, '[FORMULA_END]', '[TEXT_BEGIN]', '如图', '[TEXT_END]', '[FIGURE]', '[TEXT_BEGIN]', '[TEXT_END]', '[FORMULA_BEGIN]', 'mathord', ',', 'mathord', '[FORMULA_END]', '[TEXT_BEGIN]', '约束条件', '公式', '[TEXT_END]', '[FORMULA_BEGIN]', [FORMULA], '[FORMULA_END]', '[TEXT_BEGIN]', '[TEXT_END]', '[SEP]'], ['[TEXT_BEGIN]', '[TEXT_END]'], ['[FORMULA_BEGIN]', 'mathord', '=', 'mathord', '+', 'textord', 'mathord', '[FORMULA_END]'], ['[TEXT_BEGIN]', '最大值', '[TEXT_END]'], ['[MARK]']]\n"
     ]
    }
   ],
   "source": [
    "item = \"已知有公式$\\\\FormFigureID{1}$，如图$\\\\FigureID{088f15ea-xxx}$, 若$x,y$满足约束条件公式$\\\\FormFigureBase64{2}$，$\\\\SIFSep$则$z=x+7 y$的最大值为$\\\\SIFBlank$\"\n",
    "\n",
    "tokenizer = GensimSegTokenizer(symbol=\"gmas\")\n",
    "token_item = tokenizer(item)\n",
    "print(len(token_item), token_item)\n",
    "print()\n",
    "\n",
    "tokenizer = GensimSegTokenizer(symbol=\"gmas\", flatten=True)\n",
    "token_item = tokenizer(item)\n",
    "token_item = [i for i in token_item]\n",
    "print(len(token_item), token_item)\n",
    "print()\n",
    "\n",
    "# segment at Tag and Sep \n",
    "tokenizer = GensimSegTokenizer(symbol=\"gmas\", depth=2)\n",
    "token_item = tokenizer(item)\n",
    "print(len(token_item), token_item)\n",
    "print()\n",
    "\n",
    "# tag for texts and formulas in each big segment if setting depth.\n",
    "tokenizer = GensimSegTokenizer(symbol=\"gmas\", depth=2,  add_seg_mode=\"delimiter\")\n",
    "token_item = tokenizer(item)\n",
    "print(len(token_item), token_item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
